_includes:
   - "../../root.params"

experiment: 'object_language_ablation'
experiment_group_dir: '%adam_experiment_root%/%experiment%'

# Workflow Limitations Params
num_pursuit_learners_active: 24
pursuit_resource_request:
    exclude_list: "saga01,saga02,saga03,saga04,saga05,saga06,saga07,saga08,saga10,saga11,saga12,saga13,saga14,saga15,saga16,saga17,saga18,saga19,saga20,saga21,saga22,saga23,saga24,saga25,saga26,gaia01,gaia02"
    partition: ephemeral

# Configuration Params
min_num_objects: 4
max_num_objects : 6
min_language_accuracy: 0.5
max_language_accuracy: 0.9
num_language_accuracy_increment: 5
excluded_learner_types: "pbv, subset, cross_situational"
excluded_param_names: "1.0_graph_match_conf, 0.95_graph_match_conf"

# Baseline experiment params
object_language_ablation:
   num_samples: 400
   sort_learner_descriptions_by_length: true
   num_pretty_descriptions: 5

   log_hypothesis_every_n_steps: 100
   debug_learner_pickling: false

# Workflow Params
workflow_name: 'object_language_ablation_pegasus'
workflow_directory: '%adam_experiment_root%/%experiment%'
site: 'saga'
namespace: 'saga'

backend: slurm
partition: ephemeral
num_cpus: 1
num_gpus: 0
memory: '6G'  # not sure how much is needed
job_time_in_minutes: 720  # We request 12 hours as this is the maximum allotment for Ephemeral
                          # By requesting this much we can easily just restart the pegasus workflow
                          # And continue from the last checkpointed time. This is very useful for
                          # pursuit. It may also be wise to log more than every 100 instances
                          # to decrease the number of 'lost' training instances that can occur
                          # If the process is killed or errors out.